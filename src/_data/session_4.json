{
  "time": "15:30-17:00",
  "title": "事前学習モデルの構築と利用",
  "chair": "坂口慶祐 (東北大)",
  "talks": [
    {
      "id": "d-1",
      "type": "normal",
      "time": "15:30-16:00",
      "title": "(TBA)",
      "presenter": "-",
      "material": "",
      "abstract": ""
    },
    {
      "id": "d-2",
      "type": "normal",
      "time": "16:00-16:30",
      "title": "資源として見る実験プログラム（仮）",
      "presenter": "塚越駿 (名大)",
      "material": "",
      "abstract": ""
    },
    {
      "id": "d-3",
      "type": "normal",
      "time": "16:30-16:50",
      "title": "日本語BigBirdの構築",
      "presenter": "近藤瑞希, 王昊, 井手竜也, 伊藤俊太朗, Ritvik Choudhary, 栗原健太郎, 河原大輔 (早大)",
      "material": "",
      "abstract": "日本語の言語理解タスクを解くために,  日本語で事前学習されたモデルをファインチューニングすることで性能が向上することが知られている。日本語で事前学習されたモデルはいくつか公開されているが, ほぼすべてのモデルにおいて最大入力長は512トークン以下である。そのため, 日本語では長い入力長のデータに対して, 対応するモデルが存在せず適切な学習を行うことができない。この問題を解決するためにBigBird日本語版の事前学習を行った。BigBirdは最大で4096トークンの長さを扱うことができるEncoderモデルである。学習テキストとして日本語Wikipedia, CC-100, OSCARの3つを, バッチサイズ196, 60万ステップ(約10エポック)を目標として現在20万ステップ程学習した。このモデルを日本語言語理解ベンチマークJGLUEで評価した。また, 複数ノードで学習したときに得られた知見についても報告する。"
    },
    {
      "id": "d-4",
      "type": "lt",
      "time": "16:50-17:00",
      "title": "(TBA)",
      "presenter": "-",
      "material": "",
      "abstract": ""
    }
  ]
}